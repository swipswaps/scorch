#!/usr/bin/env python3

# ISC License (ISC)
#
# Copyright (c) 2019, Antonio SJ Musumeci <trapexit@spawn.link>
#
# Permission to use, copy, modify, and/or distribute this software for any
# purpose with or without fee is hereby granted, provided that the above
# copyright notice and this permission notice appear in all copies.
#
# THE SOFTWARE IS PROVIDED "AS IS" AND THE AUTHOR DISCLAIMS ALL WARRANTIES
# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF
# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR
# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES
# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN
# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF
# OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.

import argparse
import collections
import csv
import errno
import gzip
import hashlib
import io
import math
import os
import random
import re
import shlex
import shutil
import stat
import sys
import tempfile
import zlib
from collections import namedtuple
from datetime import datetime as dt
from itertools import takewhile


DEFAULT_DB     = 'scorch'
DEFAULT_DBPATH = '/var/tmp/scorch'
SUCCESS = 0
ERROR_PROCESSING = 1
ERROR_ARG = 2
ERROR_DIGEST_MISMATCH = 4
ERROR_FOUND = 8
ERROR_NOT_FOUND = 16


class Options(object):
    verbose      = 0
    hash         = 'md5'
    quote        = 0
    sort         = None
    filter       = None
    maxactions   = sys.maxsize
    maxdata      = sys.maxsize
    breakonerror = False
    diff_fields  = []


class FileInfo(object):
    digest = ''
    size   = 0
    mode   = 0
    mtime  = 0
    inode  = 0

    def __init__(self,digest,size,mode,mtime,inode):
        self.digest = digest
        self.size   = size
        self.mode   = mode
        self.mtime  = mtime
        self.inode  = inode

    def __str__(self):
        return str({'digest': self.digest,
                    'size': self.size,
                    'mode': self.mode,
                    'mtime': self.mtime,
                    'inode': self.inode})

    def __eq__(self, rhs):
        return ((self.digest == rhs.digest) and
                (self.size   == rhs.size)   and
                (self.mode   == rhs.mode)   and
                (self.mtime  == rhs.mtime)  and
                (self.inode  == rhs.inode))


class FileFilter(object):
    def __init__(self,basepath,fnfilter,fifilter):
        self.basepath = basepath
        self.fnfilter = fnfilter
        self.fifilter = fifilter


    def filter(self,filepath,fi,other=(lambda f: False)):
        common = commonprefix([self.basepath,filepath])
        if common != self.basepath:
            return True
        if self.fnfilter(filepath):
            return True
        if self.fifilter(fi):
            return True
        if other(filepath):
            return True
        return False


class NullFileHash(object):
    def hash(self,fullpath):
        return 'null:'


class Crc32FileHash(object):
    def hash(self,fullpath):
        digest = 0
        blocksize = 65536

        with open(fullpath,'rb') as f:
            buf = f.read(blocksize)
            while buf:
                digest = zlib.crc32(buf,digest)
                buf = f.read(blocksize)

        return 'crc32:{:08x}'.format(digest)


class Adler32FileHash(object):
    def hash(self,fullpath):
        digest = 0
        blocksize = 65536

        with open(fullpath,'rb') as f:
            buf = f.read(blocksize)
            while buf:
                digest = zlib.adler32(buf,digest)
                buf = f.read(blocksize)

        return 'adler32:{:08x}'.format(digest)


class HashLibFileHash(object):
    _hash = None

    def __init__(self,algo):
        self._hash = hashlib.new(algo)

    def hash(self,fullpath):
        blocksize = 65536
        with open(fullpath,'rb') as f:
            buf = f.read(blocksize)
            while buf:
                self._hash.update(buf)
                buf = f.read(blocksize)

        return '{}:{}'.format(self._hash.name,
                              self._hash.hexdigest())


def available_hashes():
    hashes = hashlib.algorithms_available
    hashes.add('adler32')
    hashes.add('crc32')
    hashes.add('null')
    return sorted(hashes)


def file_hash_factory(algo):
    if algo == 'adler32':
        return Adler32FileHash()
    if algo == 'crc32':
        return Crc32FileHash()
    if algo == 'null':
        return NullFileHash()
    return HashLibFileHash(algo)


def allnamesequal(name):
    return all(n==name[0] for n in name[1:])


def commonprefix(paths, sep='/'):
    bydirectorylevels = zip(*[p.split(sep) for p in paths])
    return sep.join(x[0] for x in takewhile(allnamesequal, bydirectorylevels))


def regex_type(pat):
    try:
        re.compile(pat)
    except:
        raise argparse.ArgumentTypeError('invalid regex')
    return pat


def print_help():
    help = \
'''
usage: scorch [<options>] <instruction> [<directory>]

scorch (Silent CORruption CHecker) is a tool to catalog files, hash
digests, and other metadata to help in discovering file corruption,
missing files, duplicates, etc.

positional arguments:
  instruction:             * add: compute and store hash digests for found files
                           * append: compute and store for newly found files
                           * backup: backs up selected database
                           * restore: restore backed up database
                           * list-backups: list database backups
                           * diff-backup: show diff between current & backup DB
                           * hashes: print available hash functions
                           * check: check stored info against files
                           * update: update metadata of changed files
                           * check+update: check and update if new
                           * cleanup: remove info of missing files
                           * delete: remove info for found files
                           * list-dups: list files w/ dup digests
                           * list-missing: list files no longer on filesystem
                           * list-solo: list files w/ no dup digests
                           * list-unhashed: list files not yet hashed
                           * list: md5sum'ish compatible listing
                           * in-db: show if files exist in DB
                           * found-in-db: print files found in DB
                           * notfound-in-db: print files not found in DB
  directory:               Directory or file to scan

optional arguments:
  -d, --db=:               File to store digests and other metadata in. See
                           docs for info. (default: /var/tmp/scorch/scorch.db)
  -v, --verbose:           Make `instruction` more verbose. Actual behavior
                           depends on the instruction. Can be used multiple
                           times.
  -q, --quote:             Shell quote/escape filenames when printed.
  -r, --restrict=:         * sticky: restrict scan to files with sticky bit
                           * readonly: restrict scan to readonly files
  -f, --fnfilter=:         Restrict actions to files which match regex.
  -F, --negate-fnfilter    Negate the fnfilter regex match.
  -s, --sort=:             Sorting routine on input & output (default: natural)
                           * random: shuffled / random
                           * natural: human-friendly sort, ascending
                           * reverse-natural: human-friendly sort, descending
                           * radix: RADIX sort, ascending
                           * reverse-radix: RADIX sort, descending
                           * time: sort by file mtime, ascending
                           * reverse-time: sort by file mtime, descending
  -m, --maxactions=:       Max actions before exiting (default: maxint)
  -M, --maxdata=:          Max bytes to process before exiting (default: maxint)
  -b, --break-on-error:    Any error or digest mismatch will cause an exit.
  -D, --diff-fields=:      Fields to use to indicate a file has 'changed' (vs.
                           bitrot / modified) and should be rehashed.
                           Combine with ','. (default: size)
                           * size
                           * inode
                           * mtime
                           * mode
  -H, --hash=:             Hash algo. Use 'scorch hashes' get available algos.
                           (default: md5)
  -h, --help:              Print this message

exit codes:
  *  0 : success, behavior executed, something found
  *  1 : processing error
  *  2 : error with command line arguments
  *  4 : hash mismatch
  *  8 : found
  * 16 : not found, nothing processed
'''
    print(help)


def build_arg_parser():
    parser = argparse.ArgumentParser(add_help=False)
    parser.add_argument('-d','--db',
                        type=str,
                        default=DEFAULT_DB)
    parser.add_argument('inst',
                        choices=['hashes',
                                 'add','append',
                                 'backup','restore',
                                 'list-backups','diff-backup',
                                 'check','update',
                                 'check+update','delete',
                                 'cleanup','list',
                                 'list-unhashed','list-dups',
                                 'list-solo','list-missing',
                                 'in-db',
                                 'found-in-db','notfound-in-db'],
                        nargs='?')
    parser.add_argument('dir',
                        type=str,
                        nargs='*')
    parser.add_argument('-v','--verbose',
                        action='count',
                        default=0)
    parser.add_argument('-q','--quote',
                        action='store_true',
                        default=False)
    parser.add_argument('-r','--restrict',
                        choices=['sticky',
                                 'readonly'])
    parser.add_argument('-f','--fnfilter',
                        type=regex_type)
    parser.add_argument('-F','--negate-fnfilter',
                        action='store_true',
                        default=False)
    parser.add_argument('-s','--sort',
                        choices=['none','radix','reverse-radix',
                                 'natural','reverse-natural','random',
                                 'time','reverse-time'],
                        default='natural')
    parser.add_argument('-m','--maxactions',
                        type=int,
                        default=sys.maxsize)
    parser.add_argument('-M','--maxdata',
                        type=str,
                        default=str(sys.maxsize))
    parser.add_argument('-b','--break-on-error',
                        action='store_true',
                        default=False)
    parser.add_argument('-D','--diff-fields',
                        type=str,
                        default='size')
    parser.add_argument('-H','--hash',
                        type=str,
                        choices=available_hashes(),
                        default='md5')
    parser.add_argument('-h','--help',
                        action='store_true',
                        default=False)

    return parser


def hash_file(filepath, algo):
    algo = algo.split(':')[0]
    filehash = file_hash_factory(algo)
    return filehash.hash(filepath)


def get_files(basepath,filefilter,db={}):
    if os.path.isfile(basepath):
        fi = get_fileinfo(basepath)
        if not fi:
            return []
        return [(basepath,fi)]

    files = []
    for (dirname,dirnames,filenames) in os.walk(basepath):
        for filename in filenames:
            filepath = os.path.join(dirname,filename)
            if filepath in db:
                continue

            fi = get_fileinfo(filepath)
            if not fi:
                continue

            if filefilter.filter(filepath,fi):
                continue

            files.append((filepath,fi))

    return files


def filter_files(files,filefilter,other=(lambda f: False)):
    if filefilter.basepath in files:
        return files

    rv = []
    for (filepath,fi) in files:
        if filefilter.filter(filepath,fi,other):
            continue
        rv.append((filepath,fi))

    return rv


def get_fileinfo(filepath):
    try:
        st = os.lstat(filepath)
        if not stat.S_ISREG(st.st_mode):
            return None
        return FileInfo(digest='',
                        size=st.st_size,
                        mode=st.st_mode,
                        mtime=st.st_mtime,
                        inode=st.st_ino)
    except:
        return None


def iso8601(timestamp):
    return dt.utcfromtimestamp(timestamp).isoformat() + 'Z'


def print_filepath(filepath,count=0,total=0,quote=False,end=''):
    if quote:
        filepath = shlex.quote(filepath)

    s = ''
    if count or total:
        padding = len(str(total))
        padded = str(count).zfill(padding)
        s = '{0}/{1} '.format(padded,total)

    s += filepath

    print(s,end=end)

    sys.stdout.flush()


def human_to_bytes(s):
    m = s[-1]
    if   m == 'K':
        i = int(s[0:-1]) * 1024
    elif m == 'M':
        i = int(s[0:-1]) * 1024 * 1024
    elif m == 'G':
        i = int(s[0:-1]) * 1024 * 1024 * 1024
    elif m == 'T':
        i = int(s[0:-1]) * 1024 * 1024 * 1024 * 1024
    else:
        i = int(s)

    return i


def humansize(nbytes):
    suffixes = ['B','KB','MB','GB','TB','PB','ZB']
    rank = int(math.log(nbytes,1024)) if nbytes else 0
    rank = min(rank, len(suffixes) - 1)
    human = nbytes / (1024.0 ** rank)
    if suffixes[rank] == 'B':
        f = int(human)
    else:
        f = '{:.2f}'.format(human)
    return '{}{}'.format(f, suffixes[rank])


def inst_hashes(opts,path,db,dbadd,dbremove):
    for hash in available_hashes():
        print(hash)
    return SUCCESS


def inst_add(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS;
    filepaths = get_files(path,opts.filter)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    total = min(opts.maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        if opts.verbose:
            print_filepath(filepath,actions,total,opts.quote)

        try:
            fi.digest = hash_file(filepath,opts.hash)
            dbadd[filepath] = fi
            rv = SUCCESS
            if opts.verbose:
                print(':',fi.digest)
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            err = err | ERROR_PROCESSING
            if opts.verbose:
                print(': ERROR -',e)
            if opts.breakonerror:
                break

    return rv | err


def inst_append(opts,path,db,dbadd,dbremote):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = get_files(path,opts.filter,db)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    total = min(opts.maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        if opts.verbose:
            print_filepath(filepath,actions,total,opts.quote)

        try:
            fi.digest = hash_file(filepath,opts.hash)
            dbadd[filepath] = fi
            rv = SUCCESS
            if opts.verbose:
                print(':',fi.digest)
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            err = err | ERROR_PROCESSING
            if opts.verbose:
                print(': ERROR -',e)
            if opts.breakonerror:
                break

    return rv | err


def fileinfo_changes(oldfi,newfi):
    changes = []

    text = ' - size: '
    if oldfi.size != newfi.size:
        text += '{} -> {}'.format(humansize(oldfi.size),humansize(newfi.size))
    else:
        text += '{} (unchanged)'.format(humansize(oldfi.size))
    changes.append(text)

    text = ' - mtime: '
    if oldfi.mtime != newfi.mtime:
        text += '{} -> {}'.format(iso8601(oldfi.mtime),iso8601(newfi.mtime))
    else:
        text += '{} (unchanged)'.format(iso8601(oldfi.mtime))
    changes.append(text)

    text = ' - mode: '
    if oldfi.mode != newfi.mode:
        text += '{0:03o} -> {1:03o}'.format(oldfi.mode,newfi.mode)
    else:
        text += '{0:03o} (unchanged)'.format(oldfi.mode)
    changes.append(text)

    text = ' - inode: '
    if oldfi.inode != newfi.inode:
        text += '{} -> {}'.format(oldfi.inode,newfi.inode)
    else:
        text += '{} (unchanged)'.format(oldfi.inode)
    changes.append(text)

    return changes


def different_files(old,new,fields):
    return ((('size'  in fields) and (old.size  != new.size))  or
            (('inode' in fields) and (old.inode != new.inode)) or
            (('mtime' in fields) and (old.mtime != new.mtime)) or
            (('mode'  in fields) and (old.mode  != new.mode)))


def inst_check(opts,path,db,dbadd,dbremove,update=False):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = filter_files(db.items(),opts.filter)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    total = min(opts.maxactions,len(filepaths))
    for (filepath,oldfi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += oldfi.size

        try:
            newfi = get_fileinfo(filepath)
            if not newfi:
                print_filepath(filepath,actions,total,opts.quote)
                print(': MISSING')
                continue

            rv = SUCCESS
            if different_files(oldfi,newfi,opts.diff_fields):
                err = err | ERROR_DIGEST_MISMATCH

                if opts.verbose:
                    changes = fileinfo_changes(oldfi,newfi)
                    print_filepath(filepath,actions,total,opts.quote)
                    print(': CHANGED{}'.format(' (updated)' if update else ''))
                    for change in changes:
                        print(change)
                    print(' - digest: {} -> '.format(oldfi.digest),end='')

                if update:
                    newfi.digest = hash_file(filepath,opts.hash)
                    if opts.verbose:
                        print('{}'.format(newfi.digest))

                    dbadd[filepath] = newfi
                else:
                    if opts.verbose:
                        print('not calculated')
            else:
                newfi.digest = hash_file(filepath,oldfi.digest)
                if newfi.digest != oldfi.digest:
                    err = err | ERROR_DIGEST_MISMATCH
                    print_filepath(filepath,actions,total,opts.quote)
                    print(': FAILED')
                    changes = fileinfo_changes(oldfi,newfi)
                    for change in changes:
                        print(change)
                    print(' - digest: {} -> {}'.format(oldfi.digest,newfi.digest))
                    if opts.breakonerror:
                        break
                else:
                    if update and oldfi != newfi:
                        if opts.verbose:
                            print_filepath(filepath,actions,total,opts.quote)
                            print(': OK (updated)')
                            changes = fileinfo_changes(oldfi,newfi)
                            for change in changes:
                                print(change)
                            print(' - digest: {} (unchanged)'.format(oldfi.digest))
                        dbadd[filepath] = newfi
                    else:
                        if opts.verbose >= 2:
                            print_filepath(filepath,actions,total,opts.quote)
                            print(': OK')
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            err = err | ERROR_PROCESSING
            print_filepath(filepath,actions,total,opts.quote)
            print(': ERROR -',e)
            if opts.breakonerror:
                break

    return rv | err


def inst_check_and_update(opts,path,db,dbadd,dbremove):
    return inst_check(opts,path,db,dbadd,dbremove,update=True)


def inst_update(opts,path,db,dbadd,dbremove):
    rv = SUCCESS
    filepaths = filter_files(db.items(),opts.filter)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    total = min(opts.maxactions,len(filepaths))
    for (filepath,oldfi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += oldfi.size

        try:
            newfi = get_fileinfo(filepath)
            if not newfi:
                print_filepath(filepath,actions,total,opts.quote)
                print(': MISSING')
                continue

            newfi.digest = oldfi.digest
            if oldfi == newfi:
                if opts.verbose >= 2:
                    print_filepath(filepath,actions,total,opts.quote)
                    print(': UNCHANGED')
                continue


            if opts.verbose:
                print_filepath(filepath,actions,total,opts.quote)

            if different_files(oldfi,newfi,opts.diff_fields):
                newfi.digest = hash_file(filepath,opts.hash)

            if opts.verbose:
                changes = fileinfo_changes(oldfi,newfi)
                if changes:
                    print(': CHANGED')
                    for change in changes:
                        print(change)
                print(' - digest: ',end='')
                if newfi.digest != oldfi.digest:
                    print('{} -> '.format(oldfi.digest),end='')
                    print('{}'.format(newfi.digest))
                else:
                    print('{} (unchanged)'.format(oldfi.digest))

            dbadd[filepath] = newfi
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            rv = rv | ERROR_PROCESSING
            print_filepath(filepath,actions,total,opts.quote)
            print(': ERROR -',e)
            if opts.breakonerror:
                break

    return rv


def inst_delete(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = filter_files(db.items(),opts.filter)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    total = min(opts.maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        rv = SUCCESS
        dbremove.append(filepath)
        if opts.verbose == 1:
            print_filepath(filepath,quote=opts.quote,end='\n')
        elif opts.verbose == 2:
            print_filepath(filepath,actions,total,opts.quote)
            print(': REMOVED')

    return rv | err


def inst_cleanup(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = filter_files(db.items(),opts.filter,os.path.exists)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    total = min(opts.maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        rv = SUCCESS
        dbremove.append(filepath)
        if opts.verbose:
            print_filepath(filepath,actions,total,opts.quote)
            print(': REMOVED')

    return rv | err


def inst_list(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = filter_files(db.items(),opts.filter)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    for (filepath,fi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        rv = SUCCESS
        if not opts.verbose:
            filepath = filepath[len(path)+1:]
            filepath = os.path.join('.',filepath)
        if opts.quote:
            filepath = shlex.quote(filepath)

        print(fi.digest,filepath)

    return rv | err


def inst_list_unhashed(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = get_files(path,opts.filter,db)
    opts.sort(filepaths)

    actions = 0
    processed = 0
    for (filepath,fi) in filepaths:
        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        rv = SUCCESS
        if not opts.verbose:
            filepath = filepath[len(path)+1:]
            filepath = os.path.join('.',filepath)

        print_filepath(filepath,quote=opts.quote,end='\n')

    return rv | err


def inst_list_dups(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    hashdb = {}
    for (filepath,fi) in db.items():
        if opts.filter.filter(filepath,fi):
            continue

        if not fi.digest in hashdb:
            hashdb[fi.digest] = []

        hashdb[fi.digest].append(filepath)

    actions = 0
    for (digest,filepaths) in hashdb.items():
        if len(filepaths) <= 1:
            continue

        if actions >= opts.maxactions:
            break

        actions += 1

        rv = SUCCESS
        if opts.quote:
            filepaths = [shlex.quote(filepath) for filepath in filepaths]
        filepaths.sort()
        if opts.verbose:
            print(digest,'\n -','\n - '.join(filepaths))
        else:
            print(digest,' '.join(filepaths))

    return rv | err


def inst_list_solo(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    hashdb = {}
    for (filepath,fi) in db.items():
        if opts.filter.filter(filepath,fi):
            continue

        if not fi.digest in hashdb:
            hashdb[fi.digest] = []

        hashdb[fi.digest].append(filepath)

    actions = 0
    for (digest,filepaths) in hashdb.items():
        if len(filepaths) > 1:
            continue

        if actions >= opts.maxactions:
            return rv

        actions += 1

        rv = SUCCESS
        if opts.quote:
            filepaths[0] = shlex.quote(filepaths[0])
        print(digest,filepaths[0])

    return rv | err


def inst_list_missing(opts,path,db,dbadd,dbremove):
    rv  = ERROR_NOT_FOUND
    err = SUCCESS
    filepaths = get_files(path,opts.filter)
    filepaths = dict(filepaths)

    actions = 0
    processed = 0
    output  = []
    for (filepath,fi) in db.items():
        if commonprefix([path,filepath]) != path:
            continue

        if filepath in filepaths:
            continue

        if actions >= opts.maxactions:
            break
        if processed >= opts.maxdata:
            break

        actions   += 1
        processed += fi.size

        rv = SUCCESS
        output.append((filepath,fi))

    opts.sort(output)
    for (filepath,fi) in output:
        if opts.quote:
            filepath = shlex.quote(filepath)
        print(filepath)

    return rv | err


def inst_in_db(opts,path,db,dbadd,dbremove):
    rv = SUCCESS
    sizedb = set()
    hashdb = {}
    for (filepath,fi) in db.items():
        if not fi.digest in hashdb:
            hashdb[fi.digest] = []

        hashdb[fi.digest].append(filepath)
        sizedb.add(fi.size)

    actions = 0
    processed = 0
    filepaths = get_files(path,opts.filter)
    total = min(opts.maxactions,len(filepaths))
    for (filepath,fi) in filepaths:
        try:
            if actions >= opts.maxactions:
                break
            if processed >= opts.maxdata:
                break

            actions   += 1
            processed += fi.size

            print_filepath(filepath,actions,total,opts.quote)
            if fi.size not in sizedb:
                rv = rv | ERROR_NOT_FOUND | ERROR_DIGEST_MISMATCH
                print(': NO')
            else:
                digest = hash_file(filepath,opts.hash)
                if digest not in hashdb:
                    rv = rv | ERROR_NOT_FOUND | ERROR_DIGEST_MISMATCH
                    print(': NO')
                else:
                    rv = rv | ERROR_FOUND
                    print(': YES')
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            rv = rv | ERROR_PROCESSING
            print(': ERROR -',e)

    return rv


def inst_found_in_db(opts,path,db,dbadd,dbremove):
    rv = SUCCESS
    sizedb = set()
    hashdb = {}
    writer = csv.writer(sys.stdout,delimiter=',')
    for (filepath,fi) in db.items():
        if not fi.digest in hashdb:
            hashdb[fi.digest] = []

        hashdb[fi.digest].append(filepath)
        sizedb.add(fi.size)

    actions = 0
    processed = 0
    filepaths = get_files(path,opts.filter)
    for (filepath,fi) in filepaths:
        try:
            if actions >= opts.maxactions:
                break
            if processed >= opts.maxdata:
                break

            actions   += 1
            processed += fi.size

            if fi.size not in sizedb:
                rv = rv | ERROR_NOT_FOUND
                continue

            digest = hash_file(filepath,opts.hash)
            if digest not in hashdb:
                rv = rv | ERROR_NOT_FOUND
                continue

            rv = rv | ERROR_FOUND
            if opts.verbose in [1,2]:
                if opts.verbose == 1:
                    filepath = os.path.join('.',filepath[len(path)+1:])
                if opts.quote:
                    filepath = shlex.quote(filepath)
                print(filepath)
            elif opts.verbose in [3,4]:
                if opts.verbose == 3:
                    filepath = os.path.join('.',filepath[len(path)+1:])
                if opts.quote:
                    filepath = shlex.quote(filepath)
                print(digest,filepath)
            elif opts.verbose >= 5:
                t = tuple([digest,filepath] + hashdb[digest])
                writer.writerow(t)
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            rv = rv | ERROR_PROCESSING
            print('Error while processing: ',e,file=sys.stderr)

    return rv


def inst_notfound_in_db(opts,path,db,dbadd,dbremove):
    rv = SUCCESS
    hashes = set()
    sizes  = set()
    for (filepath,fi) in db.items():
        hashes.add(fi.digest)
        sizes.add(fi.size)

    actions = 0
    processed = 0
    filepaths = get_files(path,opts.filter)
    for (filepath,fi) in filepaths:
        try:
            if actions >= opts.maxactions:
                break
            if processed >= opts.maxdata:
                break

            actions   += 1
            processed += fi.size

            if opts.verbose == 0:
                if fi.size in sizes:
                    digest = hash_file(filepath,opts.hash)
                    if digest in hashes:
                        rv = rv | ERROR_FOUND
                    else:
                        rv = rv | ERROR_NOT_FOUND
            elif opts.verbose in [1,2]:
                printpath = filepath
                if opts.verbose == 1:
                    printpath = os.path.join('.',filepath[len(path)+1:])
                if opts.quote:
                    printpath = shlex.quote(printpath)

                if fi.size not in sizes:
                    print(printpath)
                    rv = rv | ERROR_NOT_FOUND
                else:
                    digest = hash_file(filepath,opts.hash)
                    if digest not in hashes:
                        print(printpath)
                        rv = rv | ERROR_NOT_FOUND
                    else:
                        rv = rv | ERROR_FOUND
            elif opts.verbose in [3,4]:
                printpath = filepath
                if opts.verbose == 3:
                    printpath = os.path.join('.',filepath[len(path)+1:])
                if opts.quote:
                    printpath = shlex.quote(printpath)

                digest = hash_file(filepath,opts.hash)
                if digest not in hashes:
                    print(digest,printpath)
                    rv = rv | ERROR_NOT_FOUND
                else:
                    rv = rv | ERROR_FOUND
        except (KeyboardInterrupt,SystemExit):
            break
        except Exception as e:
            rv = rv | ERROR_PROCESSING
            print('Error while processing:',e,file=sys.stderr)

    return rv


def inst_backup(opts,path,db,dbadd,dbremove):
    timestamp  = dt.utcnow().replace(microsecond=0).isoformat() + 'Z'
    basepath   = os.path.dirname(opts.dbpath)
    filename   = os.path.basename(opts.dbpath)
    filename   = '{}.backup_{}'.format(filename,timestamp)
    tgt_dbpath = os.path.join(path,filename)

    tmp_dbpath = None
    try:
        (fd,tmp_dbpath) = tempfile.mkstemp(dir=basepath)
        os.close(fd)
        shutil.copy2(opts.dbpath,tmp_dbpath)
        os.replace(tmp_dbpath,tgt_dbpath)
        if opts.verbose:
            print(tgt_dbpath)
        return SUCCESS
    except Exception as e:
        if tmp_dbpath:
            os.remove(tmp_dbpath)
        print('Error backing up: {} - {}'.format(tgt_dbpath,e),file=sys.stderr)
        return ERROR_PROCESSING


def inst_restore(opts,path,db,dbadd,dbremove):
    basepath = os.path.dirname(opts.dbpath)

    tmp_dbpath = None
    try:
        (fd,tmp_dbpath) = tempfile.mkstemp(dir=basepath)
        os.close(fd)
        shutil.copy2(path,tmp_dbpath)
        os.replace(tmp_dbpath,opts.dbpath)
        return SUCCESS
    except Exception as e:
        if tmp_dbpath:
            os.remove(tmp_dbpath)
        print('Error restoring: {} - {}'.format(path,e),file=sys.stderr)
        return ERROR_PROCESSING


def inst_list_backups(opts,path,db,dbadd,dbremove):
    try:
        filename = os.path.basename(opts.dbpath)
        prefix   = '{}.backup_'.format(filename)

        backups = []
        for entry in os.listdir(path):
            fullpath = os.path.join(path,entry)
            if not entry.startswith(prefix):
                continue
            if not os.path.isfile(fullpath):
                continue
            backups.append(fullpath)

        backups.sort()
        for backup in backups:
            print(backup)
        return SUCCESS
    except Exception as e:
        msg = 'Error listing backups: {} - {}'.format(opts.dbpath,e)
        print(msg,file=sys.stderr)
        return ERROR_PROCESSING


def inst_diff_backup(opts,path,db,dbadd,dbremove):
    backup_db = read_db(path)

    for (k,v) in db.items():
        if k not in backup_db:
            print('* added:',k)
        elif v != backup_db[k]:
            print('* changed:',k)

    for (k,v) in backup_db.items():
        if k not in db:
            print('* removed:',k)

    return SUCCESS


def is_not_sticky(fi):
    return not bool(fi.mode & stat.S_ISVTX)


def is_not_readonly(fi):
    return not (fi.mode & (stat.S_IWUSR | stat.S_IWGRP | stat.S_IWOTH))


def restrict_fun(rtype):
    if rtype == 'sticky':
        return is_not_sticky
    elif rtype == 'readonly':
        return is_not_readonly
    return (lambda st: False)


def fnfilter_fun(regex,negate):
    if regex:
        cregex = re.compile(regex)
        if negate:
            return (lambda filepath: cregex.match(filepath) != None)
        return (lambda filepath: cregex.match(filepath) == None)
    return (lambda filepath: False)


Instruction = namedtuple('Instruction',['fun','needs_dirs','load_db'])

INSTRUCTIONS = {
    'hashes': Instruction(inst_hashes,False,False),
    'add': Instruction(inst_add,True,False),
    'append': Instruction(inst_append,True,True),
    'backup': Instruction(inst_backup,False,False),
    'restore': Instruction(inst_restore,True,False),
    'list-backups': Instruction(inst_list_backups,False,False),
    'diff-backup': Instruction(inst_diff_backup,True,True),
    'check': Instruction(inst_check,True,True),
    'update': Instruction(inst_update,True,True),
    'check+update': Instruction(inst_check_and_update,True,True),
    'delete': Instruction(inst_delete,True,False),
    'cleanup': Instruction(inst_cleanup,True,True),
    'list': Instruction(inst_list,True,True),
    'list-unhashed': Instruction(inst_list_unhashed,True,True),
    'list-dups': Instruction(inst_list_dups,True,True),
    'list-solo': Instruction(inst_list_solo,True,True),
    'list-missing': Instruction(inst_list_missing,True,True),
    'in-db': Instruction(inst_in_db,True,True),
    'found-in-db': Instruction(inst_found_in_db,True,True),
    'notfound-in-db': Instruction(inst_notfound_in_db,True,True)
}


def sort_fun(sort):
    if sort == 'radix':
        return (lambda l: l.sort())
    elif sort == 'reverse-radix':
        return (lambda l: l.sort(reverse=True))
    elif sort == 'random':
        return (lambda l: random.shuffle(l))
    elif sort == 'natural':
        cre = re.compile('(\d+)')
        sort_key = lambda s: [int(t) if t.isdigit() else t.lower()
                              for t in re.split(cre,s[0])]
        return (lambda l: l.sort(key=sort_key))
    elif sort == 'reverse-natural':
        cre = re.compile('(\d+)')
        sort_key = lambda s: [int(t) if t.isdigit() else t.lower()
                              for t in re.split(cre,s[0])]
        return (lambda l: l.sort(key=sort_key,reverse=True))
    elif sort == 'time':
        sort_key = lambda s: s[1].mtime
        return (lambda l: l.sort(key=sort_key))
    elif sort == 'reverse-time':
        sort_key = lambda s: s[1].mtime
        return (lambda l: l.sort(key=sort_key,reverse=True))
    return (lambda l: None)


def open_db(filepath):
    try:
        f = gzip.open(filepath,'rt',encoding='utf8',errors='surrogateescape',newline='')
        f.read(10)
        f.seek(0)
        return f
    except Exception as e:
        return open(filepath,'rt',encoding='utf8',errors='surrogateescape',newline='')


def read_db(filepath):
    db = {}
    try:
        with open_db(filepath) as f:
            reader = csv.reader(f,delimiter=',',quotechar='"')
            for row in reader:
                if len(row) == 5:
                    (filename,digest,size,mode,mtime) = row
                    inode = 0
                elif len(row) == 6:
                    (filename,digest,size,mode,mtime,inode) = row
                else:
                    raise IOError('unknown data layout in scorch database')

                if ':' not in digest:
                    digest = 'md5:'+digest
                db[filename]=FileInfo(digest,
                                      int(size),
                                      int(mode),
                                      float(mtime),
                                      int(inode))
    except (KeyboardInterrupt,SystemExit):
        raise
    except Exception as e:
        msg = 'Error reading scorch DB: {} - {}'.format(filepath,e)
        print(msg,file=sys.stderr)
    return db


def write_db_core(filepath,db):
    basepath = os.path.dirname(filepath)
    os.makedirs(basepath,mode=0o775,exist_ok=True)
    (fd,tmpfilepath) = tempfile.mkstemp(dir=basepath)
    os.close(fd)
    try:
        with gzip.open(tmpfilepath,'wt',encoding='utf8',errors='surrogateescape',newline='') as f:
            writer = csv.writer(f,delimiter=',')
            for (k,v) in db.items():
                row = (k,v.digest,v.size,v.mode,v.mtime,v.inode)
                writer.writerow(row)
        os.replace(tmpfilepath,filepath)
    except:
        os.remove(tmpfilepath)
        raise


def write_db(src_filepath,tgt_filepath,dbadd,dbremove):
    try:
        db = read_db(src_filepath)

        for k in dbremove:
            del db[k]
        for (k,v) in dbadd.items():
            db[k] = v;

        write_db_core(tgt_filepath,db)
    except (KeyboardInterrupt,SystemExit):
        raise
    except Exception as e:
        msg = 'Error writing scorch DB: {} - {}'.format(tgt_filepath,e)
        print(msg,file=sys.stderr)


def process_directories(dirs):
    rv = []
    for d in dirs:
        realpath = os.path.realpath(d)
        if realpath != '/':
            realpath + os.path.sep
        rv.append(realpath)
    return rv


def process_dbpath(dbpath):
    if dbpath[0] == '/':
        pass
    elif '/' in dbpath:
        cwd = os.getcwd()
        dbpath = os.path.join(cwd,dbpath)
    else:
        dbpath = os.path.join(DEFAULT_DBPATH,dbpath)

    if os.path.isdir(dbpath) or dbpath[-1] == '/':
        dbpath = os.path.join(dbpath,'scorch.db')

    if not os.path.splitext(dbpath)[1]:
        dbpath += '.db'

    dbpath = os.path.normpath(dbpath)

    return dbpath


def main():
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer,
                                  errors="surrogateescape",
                                  line_buffering=True)
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer,
                                  errors="surrogateescape",
                                  line_buffering=True)

    parser = build_arg_parser()
    args   = parser.parse_args()

    if args.help:
        print_help()
        parser.exit(status=SUCCESS)
    if not args.inst:
        print_help()
        parser.exit(status=ERROR_ARG)

    opts = Options()
    opts.verbose      = args.verbose
    opts.hash         = args.hash
    opts.quote        = args.quote
    opts.sort         = sort_fun(args.sort)
    opts.maxactions   = args.maxactions
    opts.maxdata      = human_to_bytes(args.maxdata)
    opts.breakonerror = args.break_on_error
    opts.dbpath       = process_dbpath(args.db)
    opts.diff_fields  = args.diff_fields.split(',')

    if not args.dir:
        if INSTRUCTIONS[args.inst].needs_dirs:
            print_help()
            parser.exit(status=ERROR_ARG)
        else:
            args.dir = [os.path.dirname(opts.dbpath)]

    func         = INSTRUCTIONS[args.inst].fun
    fnfilter     = fnfilter_fun(args.fnfilter,args.negate_fnfilter)
    fifilter     = restrict_fun(args.restrict)
    directories  = process_directories(args.dir)
    load_db      = INSTRUCTIONS[args.inst].load_db

    rv = SUCCESS
    try:
        for directory in directories:
            db       = {}
            dbadd    = {}
            dbremove = []
            if load_db:
                db = read_db(opts.dbpath)

            opts.filter = FileFilter(basepath=directory,
                                     fnfilter=fnfilter,
                                     fifilter=fifilter)

            try:
                rv = rv | func(opts,directory,db,dbadd,dbremove)
            except (KeyboardInterrupt,SystemExit):
                pass

            if len(dbadd) or len(dbremove):
                write_db(opts.dbpath,opts.dbpath,dbadd,dbremove)

            if opts.breakonerror and rv:
                break
    except (KeyboardInterrupt,SystemExit):
        pass
    except IOError as e:
        if e.errno != errno.EPIPE:
            rv = rv | ERROR_PROCESSING
            print('General processing error:',e,file=sys.stderr)
    except Exception as e:
        rv = rv | ERROR_PROCESSING
        raise

    sys.exit(rv)


if __name__ == '__main__':
    main()
